# -*- coding: utf-8 -*-
"""sentient analysis by image-checkpoint.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gFY4vNDNODVdZiwQp4i9SymGPNAXpVqW
"""

!pip install deepface
!pip install numpy
!pip install pandas
!pip install matplotlib
!pip install opencv-python

#libariry
from deepface import DeepFace
import matplotlib.pyplot as plt
import cv2

#image path
#img_path= 'happy.jpg'
#img_path= 'angry2.jpg'
img_path= 'surprise.jpg'

#load image
image=cv2.imread(img_path)
#test our image succefully loaded
print(image)

results=DeepFace.analyze(img_path,actions=['emotion'])
#test
print(results)

#display the load image
plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))
plt.axis('on')
plt.show()

max_emotion_face=max(results,key=lambda x: max(x['emotion'].values()))
#test
print(max_emotion_face)

#get the highest accuracy emotion
emotion=max_emotion_face['emotion']
max_emotion = max(emotion, key=emotion.get)
#test
print(max_emotion)

#get the bounding box coordinates of the face
x=max_emotion_face['region']['x']
y=max_emotion_face['region']['y']
w=max_emotion_face['region']['w']
h=max_emotion_face['region']['h']

#Draw a bounding box around the face
print(cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2))

#annotate the image with highest accuracy emotion
cv2.putText(image, f"{max_emotion}:{emotion[max_emotion]:.2f}%", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)

#show the image with the bounding box and annotations
plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))
plt.axis('on')
plt.show()